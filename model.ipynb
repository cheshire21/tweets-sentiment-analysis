{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REugZMxlFCvU",
        "outputId": "82050113-9289-45b6-d625-a68559d2f662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht3VaubegfgK",
        "outputId": "6ab5a76c-80a7-4cb4-d6e7-80575cf06c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed May 18 01:54:24 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLEoSsVEGzUL",
        "outputId": "b7afe855-4bfb-4d31-a636-2b5fbeca367a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 23.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 57.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 68.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n",
            "--2022-05-18 01:54:33--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz\n",
            "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 200.9.99.211, 192.80.24.4\n",
            "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|200.9.99.211|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 409871727 (391M) [application/x-gzip]\n",
            "Saving to: ‘pytorch_weights.tar.gz’\n",
            "\n",
            "pytorch_weights.tar 100%[===================>] 390.88M  7.41MB/s    in 52s     \n",
            "\n",
            "2022-05-18 01:55:25 (7.50 MB/s) - ‘pytorch_weights.tar.gz’ saved [409871727/409871727]\n",
            "\n",
            "--2022-05-18 01:55:26--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt\n",
            "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
            "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|192.80.24.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 242120 (236K) [text/plain]\n",
            "Saving to: ‘vocab.txt’\n",
            "\n",
            "vocab.txt           100%[===================>] 236.45K   466KB/s    in 0.5s    \n",
            "\n",
            "2022-05-18 01:55:27 (466 KB/s) - ‘vocab.txt’ saved [242120/242120]\n",
            "\n",
            "--2022-05-18 01:55:27--  https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json\n",
            "Resolving users.dcc.uchile.cl (users.dcc.uchile.cl)... 192.80.24.4, 200.9.99.211\n",
            "Connecting to users.dcc.uchile.cl (users.dcc.uchile.cl)|192.80.24.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 313 [application/json]\n",
            "Saving to: ‘config.json’\n",
            "\n",
            "config.json         100%[===================>]     313  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-18 01:55:27 (72.5 MB/s) - ‘config.json’ saved [313/313]\n",
            "\n",
            "pytorch/\n",
            "pytorch/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/pytorch_weights.tar.gz \n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/vocab.txt \n",
        "!wget https://users.dcc.uchile.cl/~jperez/beto/cased_2M/config.json \n",
        "!tar -xzvf pytorch_weights.tar.gz\n",
        "!mv config.json pytorch/.\n",
        "!mv vocab.txt pytorch/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2R7JQdHFGBz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import sys   \n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwmbSKjrFF_c"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/Recuperacion de la informacion/BERT/translatedEs.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMlPQF13FF9P",
        "outputId": "6f91a4b3-0a51-4029-db0b-3bd81dac5268"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((9000, 13), (1000, 13))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(train_path)\n",
        "train_df, test_df = train_test_split(df, test_size=0.1)\n",
        "train_df.shape, test_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "pqhUVSyMFF62",
        "outputId": "966f69c5-403d-420d-a72d-3d884462b83f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-87acec4e-02cd-47ff-a594-acc87908104a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Optimistic</th>\n",
              "      <th>Thankful</th>\n",
              "      <th>Empathetic</th>\n",
              "      <th>Pessimistic</th>\n",
              "      <th>Anxious</th>\n",
              "      <th>Sad</th>\n",
              "      <th>Annoyed</th>\n",
              "      <th>Denial</th>\n",
              "      <th>Official report</th>\n",
              "      <th>Surprise</th>\n",
              "      <th>Joking</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8863</th>\n",
              "      <td>1.245250e+18</td>\n",
              "      <td>Un puto tren por hora y todavía quiere ser put...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1889</th>\n",
              "      <td>1.245158e+18</td>\n",
              "      <td>Abril deseo: ser mejor dada la pérdida saludab...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3276</th>\n",
              "      <td>1.245175e+18</td>\n",
              "      <td>hijo de puta mi salud mental nunca ha sido tan...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2582</th>\n",
              "      <td>1.245166e+18</td>\n",
              "      <td>Dios por la subasta, pero si la Corona empeora...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>1.245141e+18</td>\n",
              "      <td>Se debería haber tenido una carta si theyve un...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87acec4e-02cd-47ff-a594-acc87908104a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-87acec4e-02cd-47ff-a594-acc87908104a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-87acec4e-02cd-47ff-a594-acc87908104a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                ID                                              Tweet  \\\n",
              "8863  1.245250e+18  Un puto tren por hora y todavía quiere ser put...   \n",
              "1889  1.245158e+18  Abril deseo: ser mejor dada la pérdida saludab...   \n",
              "3276  1.245175e+18  hijo de puta mi salud mental nunca ha sido tan...   \n",
              "2582  1.245166e+18  Dios por la subasta, pero si la Corona empeora...   \n",
              "240   1.245141e+18  Se debería haber tenido una carta si theyve un...   \n",
              "\n",
              "      Optimistic  Thankful  Empathetic  Pessimistic  Anxious  Sad  Annoyed  \\\n",
              "8863           0         0           0            0        0    0        1   \n",
              "1889           1         0           1            0        0    0        0   \n",
              "3276           0         0           0            0        1    0        1   \n",
              "2582           0         0           0            0        1    0        0   \n",
              "240            0         0           1            0        0    0        0   \n",
              "\n",
              "      Denial  Official report  Surprise  Joking  \n",
              "8863       0                0         0       0  \n",
              "1889       0                0         0       0  \n",
              "3276       0                0         0       0  \n",
              "2582       0                0         0       0  \n",
              "240        0                0         0       1  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY8o-zpmFF1z"
      },
      "outputs": [],
      "source": [
        "# dropping useless features/columns\n",
        "train_df.drop(labels=['ID'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nY-51AnIFd5",
        "outputId": "9f0068c8-2346-4186-ae03-0f528c981abc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Tweet', 'Optimistic', 'Thankful', 'Empathetic', 'Pessimistic',\n",
              "       'Anxious', 'Sad', 'Annoyed', 'Denial', 'Official report', 'Surprise',\n",
              "       'Joking'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkRnczi6G0ck"
      },
      "outputs": [],
      "source": [
        "# rearranging columns\n",
        "train_df = train_df[['Tweet', 'Optimistic', 'Thankful', 'Empathetic', 'Pessimistic',\n",
        "       'Anxious', 'Sad', 'Annoyed', 'Denial', 'Official report', 'Surprise',\n",
        "       'Joking']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "OIt0M-mTIUaU",
        "outputId": "de413ccb-910e-441d-cc80-2e94a21a7996"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5baec035-c4b1-4c3b-97ce-5f0c7aec43d9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Optimistic</th>\n",
              "      <th>Thankful</th>\n",
              "      <th>Empathetic</th>\n",
              "      <th>Pessimistic</th>\n",
              "      <th>Anxious</th>\n",
              "      <th>Sad</th>\n",
              "      <th>Annoyed</th>\n",
              "      <th>Denial</th>\n",
              "      <th>Official report</th>\n",
              "      <th>Surprise</th>\n",
              "      <th>Joking</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8863</th>\n",
              "      <td>Un puto tren por hora y todavía quiere ser put...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1889</th>\n",
              "      <td>Abril deseo: ser mejor dada la pérdida saludab...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3276</th>\n",
              "      <td>hijo de puta mi salud mental nunca ha sido tan...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2582</th>\n",
              "      <td>Dios por la subasta, pero si la Corona empeora...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>Se debería haber tenido una carta si theyve un...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4884</th>\n",
              "      <td>El acetaminofeno (Tylenol) * * algo ABT ibupro...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8525</th>\n",
              "      <td>PUTAS DE MIERDA Extraño el GYM usted. FUCK YOU...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3925</th>\n",
              "      <td>JumiaHeroes Buenos días kenianos, que las prec...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>856</th>\n",
              "      <td>Sí tengo c- virus de la corona cancelado o- GT...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3544</th>\n",
              "      <td>um puede esta corona + cuarentena tío, simplem...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9000 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5baec035-c4b1-4c3b-97ce-5f0c7aec43d9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5baec035-c4b1-4c3b-97ce-5f0c7aec43d9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5baec035-c4b1-4c3b-97ce-5f0c7aec43d9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                  Tweet  Optimistic  Thankful  \\\n",
              "8863  Un puto tren por hora y todavía quiere ser put...           0         0   \n",
              "1889  Abril deseo: ser mejor dada la pérdida saludab...           1         0   \n",
              "3276  hijo de puta mi salud mental nunca ha sido tan...           0         0   \n",
              "2582  Dios por la subasta, pero si la Corona empeora...           0         0   \n",
              "240   Se debería haber tenido una carta si theyve un...           0         0   \n",
              "...                                                 ...         ...       ...   \n",
              "4884  El acetaminofeno (Tylenol) * * algo ABT ibupro...           0         0   \n",
              "8525  PUTAS DE MIERDA Extraño el GYM usted. FUCK YOU...           0         0   \n",
              "3925  JumiaHeroes Buenos días kenianos, que las prec...           1         0   \n",
              "856   Sí tengo c- virus de la corona cancelado o- GT...           0         0   \n",
              "3544  um puede esta corona + cuarentena tío, simplem...           0         0   \n",
              "\n",
              "      Empathetic  Pessimistic  Anxious  Sad  Annoyed  Denial  Official report  \\\n",
              "8863           0            0        0    0        1       0                0   \n",
              "1889           1            0        0    0        0       0                0   \n",
              "3276           0            0        1    0        1       0                0   \n",
              "2582           0            0        1    0        0       0                0   \n",
              "240            1            0        0    0        0       0                0   \n",
              "...          ...          ...      ...  ...      ...     ...              ...   \n",
              "4884           0            0        0    1        0       0                1   \n",
              "8525           0            0        0    0        1       0                0   \n",
              "3925           0            0        1    0        0       0                0   \n",
              "856            0            0        0    0        0       0                0   \n",
              "3544           0            0        0    1        0       0                0   \n",
              "\n",
              "      Surprise  Joking  \n",
              "8863         0       0  \n",
              "1889         0       0  \n",
              "3276         0       0  \n",
              "2582         0       0  \n",
              "240          0       1  \n",
              "...        ...     ...  \n",
              "4884         1       1  \n",
              "8525         0       0  \n",
              "3925         1       0  \n",
              "856          0       1  \n",
              "3544         0       1  \n",
              "\n",
              "[9000 rows x 12 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkrExSqJY3ma"
      },
      "outputs": [],
      "source": [
        "target_list = ['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic',\n",
        "       'Anxious', 'Sad', 'Annoyed', 'Denial', 'Official report', 'Surprise',\n",
        "       'Joking']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSjmKyYJIl2M"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 32\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 1e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMpQi7MpRPVb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import BertForMaskedLM,BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATDXZ45xIlyq"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"pytorch/\", do_lower_case=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YXXbsj0Iltp"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['Tweet']\n",
        "        # print(self.df[target_list].values)\n",
        "        self.targets = self.df[target_list].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        # print(self.targets[index])\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHyztNfdIlrJ",
        "outputId": "130dce74-54cc-4565-e8e6-e42d0805065d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((6300, 12), (2700, 12))"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_size = 0.7\n",
        "\n",
        "val_df =train_df.copy()\n",
        "train_df = train_df.sample(frac=train_size, random_state=200)\n",
        "val_df = val_df.drop(train_df.index).reset_index(drop=True)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "train_df.shape, val_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uOisCRwZusM"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJZU3TiQQ4xA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilHjQp6RSnsw"
      },
      "outputs": [],
      "source": [
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KU7dG7QoQUOg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRUtQwS5Snqa"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJJkbOuLSnnr",
        "outputId": "d8a36bd3-cbf8-4080-c0ee-2bd801d70518"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye-qQhzCWglF"
      },
      "outputs": [],
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into       \n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGw1dRqwSnlK",
        "outputId": "56d2274f-4a5f-4045-d858-64404d01b428"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at pytorch/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BERTClass(\n",
              "  (bert_model): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(31002, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (linear): Linear(in_features=768, out_features=11, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.bert_model = BertModel.from_pretrained(\"pytorch/\", return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, 11)\n",
        "    \n",
        "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
        "        output = self.bert_model(\n",
        "            input_ids, \n",
        "            attention_mask=attn_mask, \n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        output = self.linear(output_dropout)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R08BB9adUNI4"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuRmU5SXXY2u"
      },
      "outputs": [],
      "source": [
        "val_targets=[]\n",
        "val_outputs=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN93i6WKVWn7"
      },
      "outputs": [],
      "source": [
        "def train_model(n_epochs, training_loader, validation_loader, model, \n",
        "                optimizer, checkpoint_path, best_model_path):\n",
        "   \n",
        "  # initialize tracker for minimum validation loss\n",
        "  valid_loss_min = np.Inf\n",
        "   \n",
        " \n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for batch_idx, data in enumerate(training_loader):\n",
        "        #print('yyy epoch', batch_idx)\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        #if batch_idx%5000==0:\n",
        "         #   print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        print('after loss data in training', loss.item(), train_loss)\n",
        "    \n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "    \n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        " \n",
        "    model.eval()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(validation_loader, 0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      #print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics \n",
        "      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "      \n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "        \n",
        "        # save checkpoint\n",
        "      save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "        \n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0DIW98gVWlW"
      },
      "outputs": [],
      "source": [
        "\n",
        "ckpt_path = \"/content/drive/MyDrive/Recuperacion de la informacion/BERT/multi-label/curr_ckpt\"\n",
        "best_model_path = \"/content/drive/MyDrive/Recuperacion de la informacion/BERT/multi-label/best_model.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgD-KcHfVWjS",
        "outputId": "bce30436-6631-4bac-f471-84cd15cda7f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "############# Epoch 1: Training Start   #############\n",
            "after loss data in training 0.6977332830429077 0.6977332830429077\n",
            "after loss data in training 0.7015740275382996 0.6996536552906036\n",
            "after loss data in training 0.6711342930793762 0.6901472012201945\n",
            "after loss data in training 0.6751757264137268 0.6864043325185776\n",
            "after loss data in training 0.6448654532432556 0.6780965566635132\n",
            "after loss data in training 0.6360986232757568 0.6710969010988872\n",
            "after loss data in training 0.6262750029563904 0.6646937727928163\n",
            "after loss data in training 0.6160970330238342 0.6586191803216935\n",
            "after loss data in training 0.6113699078559875 0.6533692611588373\n",
            "after loss data in training 0.5812953114509583 0.6461618661880494\n",
            "after loss data in training 0.571284294128418 0.6393548141826283\n",
            "after loss data in training 0.5657535195350647 0.633221372961998\n",
            "after loss data in training 0.5573841333389282 0.6273877391448388\n",
            "after loss data in training 0.5600057244300842 0.6225747380937849\n",
            "after loss data in training 0.5428943634033203 0.6172627131144206\n",
            "after loss data in training 0.5394419431686401 0.6123989149928093\n",
            "after loss data in training 0.5194892287254333 0.6069336393300224\n",
            "after loss data in training 0.5221925377845764 0.6022258003552754\n",
            "after loss data in training 0.49634552001953125 0.5966531540218152\n",
            "after loss data in training 0.493603378534317 0.5915006652474403\n",
            "after loss data in training 0.4945039749145508 0.5868817752315884\n",
            "after loss data in training 0.47616609930992126 0.5818492445078762\n",
            "after loss data in training 0.48412129282951355 0.5776002031305562\n",
            "after loss data in training 0.48251092433929443 0.5736381498475869\n",
            "after loss data in training 0.4902068078517914 0.570300896167755\n",
            "after loss data in training 0.4657762050628662 0.566280715740644\n",
            "after loss data in training 0.46233028173446655 0.5624306996663411\n",
            "after loss data in training 0.43353599309921265 0.5578273172889436\n",
            "after loss data in training 0.4529239237308502 0.5542099588903887\n",
            "after loss data in training 0.4410552978515625 0.5504381368557612\n",
            "after loss data in training 0.4417077898979187 0.546930706308734\n",
            "after loss data in training 0.5014998316764832 0.5455109914764761\n",
            "after loss data in training 0.45730701088905334 0.5428381435798875\n",
            "after loss data in training 0.5163556337356567 0.5420592462315278\n",
            "after loss data in training 0.4705077111721039 0.5400149166584014\n",
            "after loss data in training 0.46015164256095886 0.5377964923779169\n",
            "after loss data in training 0.46822887659072876 0.5359162865458308\n",
            "after loss data in training 0.456610769033432 0.5338292992428729\n",
            "after loss data in training 0.4415229558944702 0.5314624699262472\n",
            "after loss data in training 0.4526127278804779 0.529491226375103\n",
            "after loss data in training 0.45178136229515076 0.5275958638365676\n",
            "after loss data in training 0.4661659002304077 0.5261332456554686\n",
            "after loss data in training 0.4287255108356476 0.5238679494968681\n",
            "after loss data in training 0.4666765630245209 0.5225681452588602\n",
            "after loss data in training 0.43568941950798035 0.5206375069088407\n",
            "after loss data in training 0.4205623269081116 0.5184619595175205\n",
            "after loss data in training 0.48258447647094727 0.5176986088144019\n",
            "after loss data in training 0.44168639183044434 0.5161150209605694\n",
            "after loss data in training 0.4320808947086334 0.5144000387921626\n",
            "after loss data in training 0.43190333247184753 0.5127501046657562\n",
            "after loss data in training 0.4267640709877014 0.5110641040054021\n",
            "after loss data in training 0.4429939091205597 0.5097550617960782\n",
            "after loss data in training 0.4350410997867584 0.5083453643996759\n",
            "after loss data in training 0.431295245885849 0.5069185103531235\n",
            "after loss data in training 0.46703413128852844 0.5061933398246763\n",
            "after loss data in training 0.4459410309791565 0.5051174057381491\n",
            "after loss data in training 0.38491544127464294 0.5030085993440525\n",
            "after loss data in training 0.41172918677330017 0.5014348163686947\n",
            "after loss data in training 0.4352037310600281 0.5003122556007512\n",
            "after loss data in training 0.4422873854637146 0.4993451744318006\n",
            "after loss data in training 0.39435240626335144 0.49762398151100634\n",
            "after loss data in training 0.4168447256088257 0.4963210902867776\n",
            "after loss data in training 0.3916516900062561 0.49465967123470583\n",
            "after loss data in training 0.41182470321655273 0.49336537485942217\n",
            "after loss data in training 0.4293636977672577 0.49238073367338886\n",
            "after loss data in training 0.45210471749305725 0.4917704910039899\n",
            "after loss data in training 0.4319062829017639 0.4908769953606731\n",
            "after loss data in training 0.4473994970321655 0.49023762038525387\n",
            "after loss data in training 0.39571699500083923 0.4888677562492479\n",
            "after loss data in training 0.44610917568206787 0.48825691938400245\n",
            "after loss data in training 0.39562126994132996 0.4869521919270634\n",
            "after loss data in training 0.4087243974208832 0.4858656947811442\n",
            "after loss data in training 0.41973379254341125 0.48495977831213416\n",
            "after loss data in training 0.41683363914489746 0.4840391548098742\n",
            "after loss data in training 0.41207799315452576 0.48307967265446955\n",
            "after loss data in training 0.39467382431030273 0.4819164378078358\n",
            "after loss data in training 0.43003854155540466 0.48124269889546656\n",
            "after loss data in training 0.4120510518550873 0.48035562649751296\n",
            "after loss data in training 0.40017765760421753 0.47934071549886365\n",
            "after loss data in training 0.40494775772094727 0.47841080352663967\n",
            "after loss data in training 0.4157438278198242 0.47763713715988887\n",
            "after loss data in training 0.390121728181839 0.4765698760747907\n",
            "after loss data in training 0.40065401792526245 0.4756552271814229\n",
            "after loss data in training 0.438422828912735 0.47521198434489087\n",
            "after loss data in training 0.3866223990917206 0.4741697539301477\n",
            "after loss data in training 0.3948400914669037 0.4732473159945286\n",
            "after loss data in training 0.42566829919815063 0.4727004307439955\n",
            "after loss data in training 0.43045252561569214 0.47222034091299203\n",
            "after loss data in training 0.43847084045410156 0.4718411330426674\n",
            "after loss data in training 0.39058348536491394 0.47093827029069235\n",
            "after loss data in training 0.4024123251438141 0.47018523792644096\n",
            "after loss data in training 0.40274372696876526 0.4694521780247271\n",
            "after loss data in training 0.4385741651058197 0.4691201563804378\n",
            "after loss data in training 0.4198375344276428 0.468595873168174\n",
            "after loss data in training 0.426227331161499 0.4681498885154722\n",
            "after loss data in training 0.424560546875 0.46769583287338395\n",
            "after loss data in training 0.411066472530365 0.46711202503479615\n",
            "after loss data in training 0.3705606460571289 0.46612680688196284\n",
            "after loss data in training 0.38764140009880066 0.4653340249952642\n",
            "after loss data in training 0.3947497010231018 0.4646281817555426\n",
            "after loss data in training 0.3994709551334381 0.4639830606998782\n",
            "after loss data in training 0.4086403548717499 0.4634404851525436\n",
            "after loss data in training 0.4210626184940338 0.46302904955391727\n",
            "after loss data in training 0.40043503046035767 0.46242718398571\n",
            "after loss data in training 0.4312801957130432 0.46213054600216075\n",
            "after loss data in training 0.3937263488769531 0.461485223387772\n",
            "after loss data in training 0.40248289704322815 0.4609337997770753\n",
            "after loss data in training 0.3962726593017578 0.46033508551341495\n",
            "after loss data in training 0.4263118505477905 0.4600229457430881\n",
            "after loss data in training 0.4587557315826416 0.46001142561435676\n",
            "after loss data in training 0.4156855344772339 0.45961209326177005\n",
            "after loss data in training 0.40242791175842285 0.459101520212633\n",
            "after loss data in training 0.3885376751422882 0.45847706140670075\n",
            "after loss data in training 0.3972887694835663 0.45794032200386625\n",
            "after loss data in training 0.407543808221817 0.45750209144923976\n",
            "after loss data in training 0.4328421950340271 0.4572895061353155\n",
            "after loss data in training 0.35934606194496155 0.4564523826806971\n",
            "after loss data in training 0.3937518894672394 0.45592102256871864\n",
            "after loss data in training 0.4552178978919983 0.4559151139579899\n",
            "after loss data in training 0.4335504472255707 0.4557287417352197\n",
            "after loss data in training 0.45458322763442993 0.4557192746765355\n",
            "after loss data in training 0.41978543996810913 0.4554247350477779\n",
            "after loss data in training 0.3858577609062195 0.4548591498921555\n",
            "after loss data in training 0.396320104598999 0.45438706081721064\n",
            "after loss data in training 0.40346506237983704 0.4539796848297116\n",
            "after loss data in training 0.41073358058929443 0.4536364617801845\n",
            "after loss data in training 0.43444353342056274 0.45348533636003\n",
            "after loss data in training 0.3938027322292328 0.45301906601525815\n",
            "after loss data in training 0.3833337426185608 0.4524788697098574\n",
            "after loss data in training 0.4171997606754303 0.4522074919480541\n",
            "after loss data in training 0.3923683166503906 0.451750704350362\n",
            "after loss data in training 0.3808026909828186 0.4512132194006079\n",
            "after loss data in training 0.393934965133667 0.450782555834691\n",
            "after loss data in training 0.41225141286849976 0.45049500999165976\n",
            "after loss data in training 0.4362376928329468 0.45038940023492857\n",
            "after loss data in training 0.43045127391815186 0.4502427963649523\n",
            "after loss data in training 0.4078565537929535 0.4499334077330399\n",
            "after loss data in training 0.3787357807159424 0.44941748289958267\n",
            "after loss data in training 0.40404781699180603 0.449091082425426\n",
            "after loss data in training 0.39828240871429443 0.4487281633274893\n",
            "after loss data in training 0.39200496673583984 0.4483258711530805\n",
            "after loss data in training 0.36395367980003357 0.4477317007914393\n",
            "after loss data in training 0.4197147786617279 0.4475357782590637\n",
            "after loss data in training 0.43144941329956055 0.44742406739128937\n",
            "after loss data in training 0.461444228887558 0.4475207581602292\n",
            "after loss data in training 0.43265682458877563 0.4474189503960411\n",
            "after loss data in training 0.4007629156112671 0.447101562404308\n",
            "after loss data in training 0.4207746386528015 0.44692367778436537\n",
            "after loss data in training 0.4239421486854553 0.4467694393340371\n",
            "after loss data in training 0.3623681664466858 0.4462067641814548\n",
            "after loss data in training 0.38052383065223694 0.44577177786669175\n",
            "after loss data in training 0.4179137647151947 0.4455885014643793\n",
            "after loss data in training 0.38621702790260315 0.4452004526175703\n",
            "after loss data in training 0.40258166193962097 0.4449237072235576\n",
            "after loss data in training 0.3770810663700104 0.44448601276643795\n",
            "after loss data in training 0.4299563765525818 0.4443928740727594\n",
            "after loss data in training 0.3784601092338562 0.44397292015658807\n",
            "after loss data in training 0.43385744094848633 0.44390889813628365\n",
            "after loss data in training 0.35435500741004944 0.4433456661191375\n",
            "after loss data in training 0.4870073199272156 0.443618551455438\n",
            "after loss data in training 0.41212013363838196 0.4434229091087482\n",
            "after loss data in training 0.42902517318725586 0.4433340341956526\n",
            "after loss data in training 0.37751078605651855 0.44293021058743703\n",
            "after loss data in training 0.3373316824436188 0.4422863171231455\n",
            "after loss data in training 0.3958110809326172 0.44200464902502107\n",
            "after loss data in training 0.3893595337867737 0.4416875097765979\n",
            "after loss data in training 0.4345048666000366 0.44164449993721727\n",
            "after loss data in training 0.42702367901802063 0.44155747124126965\n",
            "after loss data in training 0.3797934949398041 0.4411920039258764\n",
            "after loss data in training 0.40947166085243225 0.4410054136725032\n",
            "after loss data in training 0.35228365659713745 0.44048657298785193\n",
            "after loss data in training 0.4207611680030823 0.44037189040073116\n",
            "after loss data in training 0.366241991519928 0.4399433938754086\n",
            "after loss data in training 0.38649702072143555 0.4396362308113053\n",
            "after loss data in training 0.3926940858364105 0.43936798998287735\n",
            "after loss data in training 0.40584084391593933 0.43917749483476975\n",
            "after loss data in training 0.4146037697792053 0.43903866022993604\n",
            "after loss data in training 0.376659095287323 0.4386882132358764\n",
            "after loss data in training 0.3666953146457672 0.4382860182716859\n",
            "after loss data in training 0.44145888090133667 0.43830364528629506\n",
            "after loss data in training 0.3585038185119629 0.4378627622654424\n",
            "after loss data in training 0.42882677912712097 0.43781311400644063\n",
            "after loss data in training 0.3978467881679535 0.4375947187832795\n",
            "after loss data in training 0.3748515844345093 0.43725372348790575\n",
            "after loss data in training 0.3732900619506836 0.43690797396608294\n",
            "after loss data in training 0.3667546808719635 0.43653080572364145\n",
            "after loss data in training 0.40086817741394043 0.4363400964813436\n",
            "after loss data in training 0.4026574194431305 0.4361609333056084\n",
            "after loss data in training 0.39922577142715454 0.4359655091686854\n",
            "after loss data in training 0.44144949316978455 0.43599437224237536\n",
            "after loss data in training 0.38969025015830994 0.43575194228382\n",
            "after loss data in training 0.411739319562912 0.435626876540482\n",
            "after loss data in training 0.43794286251068115 0.4356388764677887\n",
            "after loss data in training 0.4082869589328766 0.43549788720214483\n",
            "after loss data in training 0.3483099341392517 0.43505076949413\n",
            "after loss data in training 0.4223576486110687 0.43498600867329806\n",
            "after loss data in training 0.42576292157173157 0.4349391909722749\n",
            "############# Epoch 1: Training End     #############\n",
            "############# Epoch 1: Validation Start   #############\n",
            "############# Epoch 1: Validation End     #############\n",
            "Epoch: 1 \tAvgerage Training Loss: 0.002208 \tAverage Validation Loss: 0.004669\n",
            "Validation loss decreased (inf --> 0.004669).  Saving model ...\n",
            "############# Epoch 1  Done   #############\n",
            "\n",
            "############# Epoch 2: Training Start   #############\n",
            "after loss data in training 0.39225006103515625 0.39225006103515625\n",
            "after loss data in training 0.3898719251155853 0.3910609930753708\n",
            "after loss data in training 0.39170414209365845 0.3912753760814667\n",
            "after loss data in training 0.42345568537712097 0.39932045340538025\n",
            "after loss data in training 0.3837992250919342 0.39621620774269106\n",
            "after loss data in training 0.4199642837047577 0.4001742204030355\n",
            "after loss data in training 0.41531485319137573 0.40233716794422697\n",
            "after loss data in training 0.3957783579826355 0.401517316699028\n",
            "after loss data in training 0.39638906717300415 0.4009475111961365\n",
            "after loss data in training 0.3626686930656433 0.39711962938308715\n",
            "after loss data in training 0.37787437438964844 0.39537006074732\n",
            "after loss data in training 0.3816700577735901 0.3942283938328425\n",
            "after loss data in training 0.37079161405563354 0.3924255646192111\n",
            "after loss data in training 0.4175261855125427 0.3942184661115919\n",
            "after loss data in training 0.362836629152298 0.392126343647639\n",
            "after loss data in training 0.3504362106323242 0.3895207103341818\n",
            "after loss data in training 0.3469725251197815 0.3870178759098053\n",
            "after loss data in training 0.329214870929718 0.383806597855356\n",
            "after loss data in training 0.39435726404190063 0.3843618960757004\n",
            "after loss data in training 0.36576977372169495 0.38343228995800016\n",
            "after loss data in training 0.40177902579307556 0.3843059440453847\n",
            "after loss data in training 0.3870825469493866 0.3844321532682939\n",
            "after loss data in training 0.3956620693206787 0.38492041048796277\n",
            "after loss data in training 0.36729931831359863 0.38418619831403095\n",
            "after loss data in training 0.37140846252441406 0.3836750888824463\n",
            "after loss data in training 0.3741711676120758 0.3833095534489705\n",
            "after loss data in training 0.3819892406463623 0.38326065297479983\n",
            "after loss data in training 0.34311529994010925 0.38182689036641804\n",
            "after loss data in training 0.38028302788734436 0.3817736537292086\n",
            "after loss data in training 0.4094863533973694 0.3826974103848139\n",
            "after loss data in training 0.3689689636230469 0.3822545572634666\n",
            "after loss data in training 0.3654820919036865 0.38173041772097344\n",
            "after loss data in training 0.3738095164299011 0.38149039040912275\n",
            "after loss data in training 0.4005529284477234 0.382051053292611\n",
            "after loss data in training 0.3667032718658447 0.3816125452518463\n",
            "after loss data in training 0.39420199394226074 0.38196225215991336\n",
            "after loss data in training 0.40617606043815613 0.3826166794106767\n",
            "after loss data in training 0.3694472908973694 0.3822701165550633\n",
            "after loss data in training 0.37753328680992126 0.38214865938211096\n",
            "after loss data in training 0.3902650475502014 0.38235156908631324\n",
            "after loss data in training 0.3985455334186554 0.38274654382612644\n",
            "after loss data in training 0.3833119869232178 0.38276000675700955\n",
            "after loss data in training 0.4006851017475128 0.3831768694312073\n",
            "after loss data in training 0.36550435423851013 0.382775221358646\n",
            "after loss data in training 0.37772560119628906 0.3826630075772603\n",
            "after loss data in training 0.3579423129558563 0.38212560117244715\n",
            "after loss data in training 0.42408978939056396 0.38301845624091774\n",
            "after loss data in training 0.33511900901794434 0.38202055109043914\n",
            "after loss data in training 0.3558746874332428 0.38148696203621063\n",
            "after loss data in training 0.40625452995300293 0.38198231339454647\n",
            "after loss data in training 0.3870628774166107 0.3820819322969399\n",
            "after loss data in training 0.3595392405986786 0.38164841899505025\n",
            "after loss data in training 0.3806172311306 0.3816289626202493\n",
            "after loss data in training 0.3649367392063141 0.3813198473718431\n",
            "after loss data in training 0.4039669930934906 0.38173161365769126\n",
            "after loss data in training 0.35835275053977966 0.38131413395915714\n",
            "after loss data in training 0.36536651849746704 0.38103435123175905\n",
            "after loss data in training 0.3655599355697632 0.38076755096172465\n",
            "after loss data in training 0.40319791436195374 0.381147726612576\n",
            "after loss data in training 0.4047846496105194 0.3815416753292084\n",
            "after loss data in training 0.43428826332092285 0.382406373493007\n",
            "after loss data in training 0.3848315179347992 0.38244548872593914\n",
            "after loss data in training 0.40234532952308655 0.38276135921478277\n",
            "after loss data in training 0.39072883129119873 0.38288585096597677\n",
            "after loss data in training 0.3928971290588379 0.38303987062894385\n",
            "after loss data in training 0.33897438645362854 0.38237221177780273\n",
            "after loss data in training 0.3546220362186432 0.38195803005303913\n",
            "after loss data in training 0.3389914929866791 0.38132616921382795\n",
            "after loss data in training 0.4110221564769745 0.3817565458408301\n",
            "after loss data in training 0.34490180015563965 0.38123004947389877\n",
            "after loss data in training 0.3955483138561249 0.3814317151694231\n",
            "after loss data in training 0.3379531800746918 0.3808278466264407\n",
            "after loss data in training 0.34960728883743286 0.3804001677526187\n",
            "after loss data in training 0.37030819058418274 0.38026378968277497\n",
            "after loss data in training 0.36474061012268066 0.38005681395530705\n",
            "after loss data in training 0.42840316891670227 0.3806929502047991\n",
            "after loss data in training 0.3917749226093292 0.38083687192433846\n",
            "after loss data in training 0.3818773925304413 0.380850211932109\n",
            "after loss data in training 0.40379971265792847 0.38114071194129656\n",
            "after loss data in training 0.3991892635822296 0.3813663188368082\n",
            "after loss data in training 0.36443468928337097 0.3811572863731855\n",
            "after loss data in training 0.3790248930454254 0.3811312815765055\n",
            "after loss data in training 0.3617722988128662 0.3808980408203171\n",
            "after loss data in training 0.3576413094997406 0.3806211749712626\n",
            "after loss data in training 0.40297526121139526 0.3808841642211465\n",
            "after loss data in training 0.3603862226009369 0.380645816062772\n",
            "after loss data in training 0.3828442394733429 0.38067108529737625\n",
            "after loss data in training 0.37707415223121643 0.38063021105798805\n",
            "after loss data in training 0.37982842326164246 0.3806212022063437\n",
            "after loss data in training 0.3721585273742676 0.38052717248598733\n",
            "after loss data in training 0.3558677136898041 0.380256189422293\n",
            "after loss data in training 0.38088393211364746 0.3802630127124164\n",
            "after loss data in training 0.37428033351898193 0.380198682828616\n",
            "after loss data in training 0.3612179160118103 0.3799967597773734\n",
            "after loss data in training 0.3746105134487152 0.37994006244759804\n",
            "after loss data in training 0.3679036796092987 0.3798146834596991\n",
            "after loss data in training 0.38175106048583984 0.3798346461094531\n",
            "after loss data in training 0.36880096793174744 0.37972205755661936\n",
            "after loss data in training 0.36519989371299744 0.3795753690329464\n",
            "after loss data in training 0.3941154181957245 0.3797207695245742\n",
            "after loss data in training 0.40322452783584595 0.3799534800029036\n",
            "after loss data in training 0.39746609330177307 0.3801251722901474\n",
            "after loss data in training 0.3994212746620178 0.3803125130898743\n",
            "after loss data in training 0.3482133150100708 0.38000386695449156\n",
            "after loss data in training 0.41461217403411865 0.3803334698790594\n",
            "after loss data in training 0.3531309962272644 0.38007684276913684\n",
            "after loss data in training 0.328269362449646 0.3795926607100762\n",
            "after loss data in training 0.3810201585292816 0.3796058782824762\n",
            "after loss data in training 0.39217716455459595 0.3797212111840553\n",
            "after loss data in training 0.35055413842201233 0.3794560559771276\n",
            "after loss data in training 0.34942522644996643 0.3791855079633694\n",
            "after loss data in training 0.37140652537345886 0.37911605276167376\n",
            "after loss data in training 0.3940073549747467 0.37924783419718766\n",
            "after loss data in training 0.3973202705383301 0.379406364340531\n",
            "after loss data in training 0.3643770217895508 0.3792756744053051\n",
            "after loss data in training 0.3671857714653015 0.37917145110409817\n",
            "after loss data in training 0.3979661762714386 0.3793320897807421\n",
            "after loss data in training 0.3938685953617096 0.3794552805060046\n",
            "after loss data in training 0.36195316910743713 0.37930820393963005\n",
            "after loss data in training 0.36484494805336 0.37918767680724447\n",
            "after loss data in training 0.3908872604370117 0.3792843675810442\n",
            "after loss data in training 0.40364083647727966 0.3794840107687183\n",
            "after loss data in training 0.363277792930603 0.3793522529001157\n",
            "after loss data in training 0.3580792546272278 0.37918069646243113\n",
            "after loss data in training 0.3709561824798584 0.37911490035057055\n",
            "after loss data in training 0.402252733707428 0.3792985339486408\n",
            "after loss data in training 0.35220736265182495 0.3790852176392171\n",
            "after loss data in training 0.4049319326877594 0.37928714510053385\n",
            "after loss data in training 0.3619316518306732 0.37915260639301557\n",
            "after loss data in training 0.3840780258178711 0.3791904942347452\n",
            "after loss data in training 0.3769334852695465 0.379173265158675\n",
            "after loss data in training 0.3863276541233063 0.3792274650750737\n",
            "after loss data in training 0.36666056513786316 0.379132977105621\n",
            "after loss data in training 0.3779172897338867 0.3791239048118021\n",
            "after loss data in training 0.3384784162044525 0.3788228271184143\n",
            "after loss data in training 0.3450475335121155 0.37857447937130917\n",
            "after loss data in training 0.4016927480697632 0.37874322585815917\n",
            "after loss data in training 0.3825434744358063 0.3787707638913305\n",
            "after loss data in training 0.41794487833976746 0.37905259205283004\n",
            "after loss data in training 0.33221435546875 0.3787180332200866\n",
            "after loss data in training 0.4090227782726288 0.37893296048996283\n",
            "after loss data in training 0.3694835603237152 0.3788664154183695\n",
            "after loss data in training 0.3835814297199249 0.37889938754635244\n",
            "after loss data in training 0.3939189016819 0.3790036897278493\n",
            "after loss data in training 0.3371821343898773 0.3787152652082771\n",
            "after loss data in training 0.3530937731266022 0.37853977553648477\n",
            "after loss data in training 0.378110408782959 0.3785368546742159\n",
            "after loss data in training 0.34050413966178894 0.37827987687007786\n",
            "after loss data in training 0.3632260859012604 0.3781788447159247\n",
            "after loss data in training 0.363156795501709 0.37807869772116326\n",
            "after loss data in training 0.3338111639022827 0.37778553524554154\n",
            "after loss data in training 0.34389016032218933 0.3775625393578879\n",
            "after loss data in training 0.39023643732070923 0.3776453752922854\n",
            "after loss data in training 0.3711661994457245 0.3776033027218532\n",
            "after loss data in training 0.3121025562286377 0.37718071726060665\n",
            "after loss data in training 0.36098146438598633 0.37707687589602573\n",
            "after loss data in training 0.4242156445980072 0.37737712283043323\n",
            "after loss data in training 0.3380222022533417 0.3771280410546289\n",
            "after loss data in training 0.37564754486083984 0.3771187297578126\n",
            "after loss data in training 0.4022109806537628 0.37727555632591225\n",
            "after loss data in training 0.4069603383541107 0.3774599338540377\n",
            "after loss data in training 0.39497366547584534 0.3775680433084933\n",
            "after loss data in training 0.38678765296936035 0.3776246053309526\n",
            "after loss data in training 0.39267975091934204 0.37771640499917447\n",
            "after loss data in training 0.3615086078643799 0.37761817592563024\n",
            "after loss data in training 0.37809598445892334 0.37762105429028864\n",
            "after loss data in training 0.38331684470176697 0.3776551608196987\n",
            "after loss data in training 0.39572834968566895 0.37776273932485327\n",
            "after loss data in training 0.3938939869403839 0.37785819049417596\n",
            "after loss data in training 0.3152185082435608 0.3774897217750547\n",
            "after loss data in training 0.38345155119895935 0.37752458627460966\n",
            "after loss data in training 0.35346719622612 0.3773847177278161\n",
            "after loss data in training 0.33744725584983826 0.37715386534701856\n",
            "after loss data in training 0.36783885955810547 0.3771003308309903\n",
            "after loss data in training 0.3409000635147095 0.3768934721606116\n",
            "after loss data in training 0.4187924861907959 0.37713153474032857\n",
            "after loss data in training 0.37343183159828186 0.3771106324626899\n",
            "after loss data in training 0.36305421590805054 0.37703166383036046\n",
            "after loss data in training 0.385383278131485 0.37707832089349524\n",
            "after loss data in training 0.3592960238456726 0.3769795303543407\n",
            "after loss data in training 0.3180549442768097 0.3766539801550173\n",
            "after loss data in training 0.3395346403121948 0.3764500277382985\n",
            "after loss data in training 0.3810599148273468 0.37647521837812936\n",
            "after loss data in training 0.39332690834999084 0.37656680364971556\n",
            "after loss data in training 0.3735290467739105 0.3765503833422788\n",
            "after loss data in training 0.39815643429756165 0.37666654490655455\n",
            "after loss data in training 0.36033400893211365 0.37657920514198534\n",
            "after loss data in training 0.3558812737464905 0.3764691097622221\n",
            "after loss data in training 0.3376363515853882 0.37626364543324414\n",
            "after loss data in training 0.37385091185569763 0.37625094683546756\n",
            "after loss data in training 0.3634302020072937 0.37618382251699545\n",
            "after loss data in training 0.34404119849205017 0.3760164130168655\n",
            "after loss data in training 0.3557737469673157 0.37591152873681605\n",
            "after loss data in training 0.3379037380218506 0.3757156122898317\n",
            "after loss data in training 0.2994239628314972 0.37532437306184024\n",
            "after loss data in training 0.349457710981369 0.37519240029612355\n",
            "after loss data in training 0.36979159712791443 0.3751649850516149\n",
            "############# Epoch 2: Training End     #############\n",
            "############# Epoch 2: Validation Start   #############\n",
            "############# Epoch 2: Validation End     #############\n",
            "Epoch: 2 \tAvgerage Training Loss: 0.001904 \tAverage Validation Loss: 0.004264\n",
            "Validation loss decreased (0.004669 --> 0.004264).  Saving model ...\n",
            "############# Epoch 2  Done   #############\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trained_model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)\n",
        "# train_data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L9o8grxVWgc",
        "outputId": "c2be57a6-805b-4d71-937f-5c3cb6aa5cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimado Gracias por estar ahí para mí durante la mitad de la crisis coronavirus. Amor, tu mayor fan\n",
            "ID                                             1245208599929987072.0\n",
            "Tweet              cualquier otra persona que espera en China par...\n",
            "Optimistic                                                         0\n",
            "Thankful                                                           0\n",
            "Empathetic                                                         0\n",
            "Pessimistic                                                        0\n",
            "Anxious                                                            0\n",
            "Sad                                                                0\n",
            "Annoyed                                                            0\n",
            "Denial                                                             0\n",
            "Official report                                                    0\n",
            "Surprise                                                           0\n",
            "Joking                                                             1\n",
            "Name: 5962, dtype: object\n",
            "[['Optimistic' 'Thankful' 'Empathetic' 'Pessimistic' 'Anxious' 'Sad'\n",
            "  'Annoyed' 'Denial' 'Official report' 'Surprise' 'Joking']\n",
            " [0.48262903094291687 0.17602801322937012 0.16808073222637177\n",
            "  0.06030616536736488 0.12755639851093292 0.2523386478424072\n",
            "  0.1586076021194458 0.03371649608016014 0.04623796045780182\n",
            "  0.07658012956380844 0.32453662157058716]]\n",
            "data\n",
            "[[0.48262903094291687, 0.17602801322937012, 0.16808073222637177, 0.06030616536736488, 0.12755639851093292, 0.2523386478424072, 0.1586076021194458, 0.03371649608016014, 0.04623796045780182, 0.07658012956380844, 0.32453662157058716]]\n",
            "\n",
            "[[ 7  8  3  9  4  6  2  1  5 10  0]]\n",
            "\n",
            "[[0.61836849 0.54389372 0.54192154 0.51507197 0.53184593 0.56275204\n",
            "  0.53956898 0.50842833 0.51155743 0.51913568 0.58042946]]\n",
            "\n",
            "\n",
            "Optimistic\n"
          ]
        }
      ],
      "source": [
        "# testing\n",
        "example = test_df.iloc[1]['Tweet']\n",
        "print(example)\n",
        "print(test_df.iloc[0])\n",
        "encodings = tokenizer.encode_plus(\n",
        "    example,\n",
        "    None,\n",
        "    add_special_tokens=True,\n",
        "    max_length=MAX_LEN,\n",
        "    padding='max_length',\n",
        "    return_token_type_ids=True,\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "    attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "    token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "    output = model(input_ids, attention_mask, token_type_ids)\n",
        "    final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n",
        "    \n",
        "    print(np.concatenate((np.array([train_df.columns[1:]]),final_output),axis=0))\n",
        "    print('data')\n",
        "    print(final_output)\n",
        "    print()\n",
        "    print(np.argsort(final_output, axis=1))\n",
        "    print()\n",
        "    print((1/(1 + np.exp(-1 * np.array(final_output)))))\n",
        "    print()\n",
        "    print()\n",
        "    print(train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "bEyp1AXttYsD",
        "outputId": "6888eacf-68c1-4a4c-829d-20df6fbd1d8e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e849e347-46fc-43ec-8f8a-0a4b29121fa8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>datetime</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_stop_word</th>\n",
              "      <th>clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2020-01-04 09:57:06</td>\n",
              "      <td>No es hora de lamentos y tampoco de críticas.H...</td>\n",
              "      <td>no es hora de lamentos tampoco de críticas hay...</td>\n",
              "      <td>no hora lamentos tampoco críticas hecho real c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2020-01-05 12:56:25</td>\n",
              "      <td>@EleccionesCong Quiere aparentar distanciamien...</td>\n",
              "      <td>eleccionescong quiere aparentar distanciamien...</td>\n",
              "      <td>eleccionescong quiere aparentar distanciamient...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2020-01-05 21:32:35</td>\n",
              "      <td>@5ojgonzalez Maduro miente una vez más. Eligió...</td>\n",
              "      <td>ojgonzalez maduro miente una vez más eligió r...</td>\n",
              "      <td>ojgonzalez maduro miente vez eligió repetir gu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2020-01-11 08:23:26</td>\n",
              "      <td>Creo que en este instante hay como una epidemi...</td>\n",
              "      <td>creo que en este instante hay como una epidemi...</td>\n",
              "      <td>creo instante epidemia diarrea ciudad siento m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2020-01-15 20:09:51</td>\n",
              "      <td>@Cecilia605040 Se contagió del virus Arbizu</td>\n",
              "      <td>cecilia se contagió del virus arbizu</td>\n",
              "      <td>cecilia contagió virus arbizu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41182</th>\n",
              "      <td>41184</td>\n",
              "      <td>2021-12-30 21:01:43</td>\n",
              "      <td>Estupefacto, pedí hablar con un médico y me at...</td>\n",
              "      <td>estupefacto pedí hablar con un médico me atend...</td>\n",
              "      <td>estupefacto pedí hablar médico atendió señorit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41183</th>\n",
              "      <td>41185</td>\n",
              "      <td>2021-12-30 21:01:47</td>\n",
              "      <td>Cuánta gente se contagiará, quedará con secuel...</td>\n",
              "      <td>cuánta gente se contagiará quedará con secuela...</td>\n",
              "      <td>cuánta gente contagiará quedará secuelas morir...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41184</th>\n",
              "      <td>41186</td>\n",
              "      <td>2021-12-30 22:17:13</td>\n",
              "      <td>@patriciagamarra Hay empresas que saben que su...</td>\n",
              "      <td>patriciagamarra hay empresas que saben que su...</td>\n",
              "      <td>patriciagamarra empresas saben personal covid ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41185</th>\n",
              "      <td>41187</td>\n",
              "      <td>2021-12-30 22:43:44</td>\n",
              "      <td>Quiero torta de chocolate, puedo comer torta d...</td>\n",
              "      <td>quiero torta de chocolate puedo comer torta de...</td>\n",
              "      <td>quiero torta chocolate puedo comer torta choco...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41186</th>\n",
              "      <td>41188</td>\n",
              "      <td>2021-12-30 23:46:08</td>\n",
              "      <td>Enserio con toda esto del Covid solo deseo ade...</td>\n",
              "      <td>enserio con toda esto del covid solo deseo ade...</td>\n",
              "      <td>enserio toda covid solo deseo adelantar viaje ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41187 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e849e347-46fc-43ec-8f8a-0a4b29121fa8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e849e347-46fc-43ec-8f8a-0a4b29121fa8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e849e347-46fc-43ec-8f8a-0a4b29121fa8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Unnamed: 0             datetime  \\\n",
              "0               0  2020-01-04 09:57:06   \n",
              "1               1  2020-01-05 12:56:25   \n",
              "2               2  2020-01-05 21:32:35   \n",
              "3               3  2020-01-11 08:23:26   \n",
              "4               4  2020-01-15 20:09:51   \n",
              "...           ...                  ...   \n",
              "41182       41184  2021-12-30 21:01:43   \n",
              "41183       41185  2021-12-30 21:01:47   \n",
              "41184       41186  2021-12-30 22:17:13   \n",
              "41185       41187  2021-12-30 22:43:44   \n",
              "41186       41188  2021-12-30 23:46:08   \n",
              "\n",
              "                                                   tweet  \\\n",
              "0      No es hora de lamentos y tampoco de críticas.H...   \n",
              "1      @EleccionesCong Quiere aparentar distanciamien...   \n",
              "2      @5ojgonzalez Maduro miente una vez más. Eligió...   \n",
              "3      Creo que en este instante hay como una epidemi...   \n",
              "4            @Cecilia605040 Se contagió del virus Arbizu   \n",
              "...                                                  ...   \n",
              "41182  Estupefacto, pedí hablar con un médico y me at...   \n",
              "41183  Cuánta gente se contagiará, quedará con secuel...   \n",
              "41184  @patriciagamarra Hay empresas que saben que su...   \n",
              "41185  Quiero torta de chocolate, puedo comer torta d...   \n",
              "41186  Enserio con toda esto del Covid solo deseo ade...   \n",
              "\n",
              "                                         tweet_stop_word  \\\n",
              "0      no es hora de lamentos tampoco de críticas hay...   \n",
              "1       eleccionescong quiere aparentar distanciamien...   \n",
              "2       ojgonzalez maduro miente una vez más eligió r...   \n",
              "3      creo que en este instante hay como una epidemi...   \n",
              "4                   cecilia se contagió del virus arbizu   \n",
              "...                                                  ...   \n",
              "41182  estupefacto pedí hablar con un médico me atend...   \n",
              "41183  cuánta gente se contagiará quedará con secuela...   \n",
              "41184   patriciagamarra hay empresas que saben que su...   \n",
              "41185  quiero torta de chocolate puedo comer torta de...   \n",
              "41186  enserio con toda esto del covid solo deseo ade...   \n",
              "\n",
              "                                                   clean  \n",
              "0      no hora lamentos tampoco críticas hecho real c...  \n",
              "1      eleccionescong quiere aparentar distanciamient...  \n",
              "2      ojgonzalez maduro miente vez eligió repetir gu...  \n",
              "3      creo instante epidemia diarrea ciudad siento m...  \n",
              "4                          cecilia contagió virus arbizu  \n",
              "...                                                  ...  \n",
              "41182  estupefacto pedí hablar médico atendió señorit...  \n",
              "41183  cuánta gente contagiará quedará secuelas morir...  \n",
              "41184  patriciagamarra empresas saben personal covid ...  \n",
              "41185  quiero torta chocolate puedo comer torta choco...  \n",
              "41186  enserio toda covid solo deseo adelantar viaje ...  \n",
              "\n",
              "[41187 rows x 5 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add validation data  \n",
        "df_clean_data = pd.read_excel('/content/df_final_clean.xlsx')\n",
        "\n",
        "df_clean_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG25zHmb3lrn",
        "outputId": "f9703302-4da1-4243-a643-8285cf8bf824"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Unnamed: 0         0\n",
              "datetime           0\n",
              "tweet              0\n",
              "tweet_stop_word    0\n",
              "clean              0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_clean_data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS3t5LkxZzrb"
      },
      "outputs": [],
      "source": [
        "# testing\n",
        "def label_data_single(df):\n",
        "  size = df.shape[0]\n",
        "  labels = np.zeros((size,), dtype=np.int)\n",
        "  names = np.chararray((size,), itemsize=20)\n",
        "  for i in range(size):\n",
        "\n",
        "    text = df.iloc[i]['tweet_stop_word']\n",
        "\n",
        "    encodings = tokenizer.encode_plus(\n",
        "        text,\n",
        "        None,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding='max_length',\n",
        "        return_token_type_ids=True,\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "        attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "        token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "        output = model(input_ids, attention_mask, token_type_ids)\n",
        "        final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n",
        "        \n",
        "        index = int(np.argmax(final_output, axis=1))\n",
        "        name = train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))]\n",
        "\n",
        "        labels[i] = index\n",
        "        names[i] = name\n",
        "\n",
        "  df['label'] = labels\n",
        "  df['name'] = names\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMwplgr20CD0",
        "outputId": "33f3b2e4-fac4-4b90-e60f-7e739f81942c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Unnamed: 0         41189\n",
              "Unnamed: 0.1       41189\n",
              "datetime           41189\n",
              "tweet_original     41189\n",
              "tweet_stop_word    40914\n",
              "tweet_clean        40889\n",
              "hashtag             9564\n",
              "dtype: int64"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_clean_data[df_clean_data[\"tweet_stop_word\"].isna()].to_csv('nan_rows.csv')\n",
        "df_clean_data.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "id": "TyGsFCrByuRk",
        "outputId": "f8d5d9af-84e8-452b-93ef-6f64b7ff3491"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f1ff8027-4a3b-4b31-9fa3-32346f949f72\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>datetime</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_stop_word</th>\n",
              "      <th>clean</th>\n",
              "      <th>label</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2020-01-04 09:57:06</td>\n",
              "      <td>No es hora de lamentos y tampoco de críticas.H...</td>\n",
              "      <td>no es hora de lamentos tampoco de críticas hay...</td>\n",
              "      <td>no hora lamentos tampoco críticas hecho real c...</td>\n",
              "      <td>6</td>\n",
              "      <td>b'Annoyed'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2020-01-05 12:56:25</td>\n",
              "      <td>@EleccionesCong Quiere aparentar distanciamien...</td>\n",
              "      <td>eleccionescong quiere aparentar distanciamien...</td>\n",
              "      <td>eleccionescong quiere aparentar distanciamient...</td>\n",
              "      <td>6</td>\n",
              "      <td>b'Annoyed'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2020-01-05 21:32:35</td>\n",
              "      <td>@5ojgonzalez Maduro miente una vez más. Eligió...</td>\n",
              "      <td>ojgonzalez maduro miente una vez más eligió r...</td>\n",
              "      <td>ojgonzalez maduro miente vez eligió repetir gu...</td>\n",
              "      <td>6</td>\n",
              "      <td>b'Annoyed'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2020-01-11 08:23:26</td>\n",
              "      <td>Creo que en este instante hay como una epidemi...</td>\n",
              "      <td>creo que en este instante hay como una epidemi...</td>\n",
              "      <td>creo instante epidemia diarrea ciudad siento m...</td>\n",
              "      <td>10</td>\n",
              "      <td>b'Joking'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2020-01-15 20:09:51</td>\n",
              "      <td>@Cecilia605040 Se contagió del virus Arbizu</td>\n",
              "      <td>cecilia se contagió del virus arbizu</td>\n",
              "      <td>cecilia contagió virus arbizu</td>\n",
              "      <td>10</td>\n",
              "      <td>b'Joking'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41182</th>\n",
              "      <td>41184</td>\n",
              "      <td>2021-12-30 21:01:43</td>\n",
              "      <td>Estupefacto, pedí hablar con un médico y me at...</td>\n",
              "      <td>estupefacto pedí hablar con un médico me atend...</td>\n",
              "      <td>estupefacto pedí hablar médico atendió señorit...</td>\n",
              "      <td>10</td>\n",
              "      <td>b'Joking'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41183</th>\n",
              "      <td>41185</td>\n",
              "      <td>2021-12-30 21:01:47</td>\n",
              "      <td>Cuánta gente se contagiará, quedará con secuel...</td>\n",
              "      <td>cuánta gente se contagiará quedará con secuela...</td>\n",
              "      <td>cuánta gente contagiará quedará secuelas morir...</td>\n",
              "      <td>0</td>\n",
              "      <td>b'Optimistic'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41184</th>\n",
              "      <td>41186</td>\n",
              "      <td>2021-12-30 22:17:13</td>\n",
              "      <td>@patriciagamarra Hay empresas que saben que su...</td>\n",
              "      <td>patriciagamarra hay empresas que saben que su...</td>\n",
              "      <td>patriciagamarra empresas saben personal covid ...</td>\n",
              "      <td>10</td>\n",
              "      <td>b'Joking'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41185</th>\n",
              "      <td>41187</td>\n",
              "      <td>2021-12-30 22:43:44</td>\n",
              "      <td>Quiero torta de chocolate, puedo comer torta d...</td>\n",
              "      <td>quiero torta de chocolate puedo comer torta de...</td>\n",
              "      <td>quiero torta chocolate puedo comer torta choco...</td>\n",
              "      <td>10</td>\n",
              "      <td>b'Joking'</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41186</th>\n",
              "      <td>41188</td>\n",
              "      <td>2021-12-30 23:46:08</td>\n",
              "      <td>Enserio con toda esto del Covid solo deseo ade...</td>\n",
              "      <td>enserio con toda esto del covid solo deseo ade...</td>\n",
              "      <td>enserio toda covid solo deseo adelantar viaje ...</td>\n",
              "      <td>6</td>\n",
              "      <td>b'Annoyed'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41187 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1ff8027-4a3b-4b31-9fa3-32346f949f72')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f1ff8027-4a3b-4b31-9fa3-32346f949f72 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f1ff8027-4a3b-4b31-9fa3-32346f949f72');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Unnamed: 0             datetime  \\\n",
              "0               0  2020-01-04 09:57:06   \n",
              "1               1  2020-01-05 12:56:25   \n",
              "2               2  2020-01-05 21:32:35   \n",
              "3               3  2020-01-11 08:23:26   \n",
              "4               4  2020-01-15 20:09:51   \n",
              "...           ...                  ...   \n",
              "41182       41184  2021-12-30 21:01:43   \n",
              "41183       41185  2021-12-30 21:01:47   \n",
              "41184       41186  2021-12-30 22:17:13   \n",
              "41185       41187  2021-12-30 22:43:44   \n",
              "41186       41188  2021-12-30 23:46:08   \n",
              "\n",
              "                                                   tweet  \\\n",
              "0      No es hora de lamentos y tampoco de críticas.H...   \n",
              "1      @EleccionesCong Quiere aparentar distanciamien...   \n",
              "2      @5ojgonzalez Maduro miente una vez más. Eligió...   \n",
              "3      Creo que en este instante hay como una epidemi...   \n",
              "4            @Cecilia605040 Se contagió del virus Arbizu   \n",
              "...                                                  ...   \n",
              "41182  Estupefacto, pedí hablar con un médico y me at...   \n",
              "41183  Cuánta gente se contagiará, quedará con secuel...   \n",
              "41184  @patriciagamarra Hay empresas que saben que su...   \n",
              "41185  Quiero torta de chocolate, puedo comer torta d...   \n",
              "41186  Enserio con toda esto del Covid solo deseo ade...   \n",
              "\n",
              "                                         tweet_stop_word  \\\n",
              "0      no es hora de lamentos tampoco de críticas hay...   \n",
              "1       eleccionescong quiere aparentar distanciamien...   \n",
              "2       ojgonzalez maduro miente una vez más eligió r...   \n",
              "3      creo que en este instante hay como una epidemi...   \n",
              "4                   cecilia se contagió del virus arbizu   \n",
              "...                                                  ...   \n",
              "41182  estupefacto pedí hablar con un médico me atend...   \n",
              "41183  cuánta gente se contagiará quedará con secuela...   \n",
              "41184   patriciagamarra hay empresas que saben que su...   \n",
              "41185  quiero torta de chocolate puedo comer torta de...   \n",
              "41186  enserio con toda esto del covid solo deseo ade...   \n",
              "\n",
              "                                                   clean  label           name  \n",
              "0      no hora lamentos tampoco críticas hecho real c...      6     b'Annoyed'  \n",
              "1      eleccionescong quiere aparentar distanciamient...      6     b'Annoyed'  \n",
              "2      ojgonzalez maduro miente vez eligió repetir gu...      6     b'Annoyed'  \n",
              "3      creo instante epidemia diarrea ciudad siento m...     10      b'Joking'  \n",
              "4                          cecilia contagió virus arbizu     10      b'Joking'  \n",
              "...                                                  ...    ...            ...  \n",
              "41182  estupefacto pedí hablar médico atendió señorit...     10      b'Joking'  \n",
              "41183  cuánta gente contagiará quedará secuelas morir...      0  b'Optimistic'  \n",
              "41184  patriciagamarra empresas saben personal covid ...     10      b'Joking'  \n",
              "41185  quiero torta chocolate puedo comer torta choco...     10      b'Joking'  \n",
              "41186  enserio toda covid solo deseo adelantar viaje ...      6     b'Annoyed'  \n",
              "\n",
              "[41187 rows x 7 columns]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_1 = df_clean_data.copy()\n",
        "# df_1\n",
        "label_data_single(df_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKcb_9b9y0IB"
      },
      "outputs": [],
      "source": [
        "df_1.to_excel('df_labeled_final.xlsx')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "i7_Db97y2ZoW",
        "outputId": "c67a4101-a62c-4114-86ee-cf7a0f59e070"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-84a70628-4155-40e5-8ac5-dc653b66a62c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Date</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2021-12-30 23:46:08+00:00</td>\n",
              "      <td>enserio con toda esto del covid solo deseo ade...</td>\n",
              "      <td>6</td>\n",
              "      <td>Annoyed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2021-12-30 22:43:44+00:00</td>\n",
              "      <td>quiero torta de chocolate puedo comer torta de...</td>\n",
              "      <td>10</td>\n",
              "      <td>Joking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2021-12-30 22:17:13+00:00</td>\n",
              "      <td>patriciagamarra hay empresas que saben que su ...</td>\n",
              "      <td>10</td>\n",
              "      <td>Joking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2021-12-30 21:01:47+00:00</td>\n",
              "      <td>cuánta gente se contagiará quedará con secuela...</td>\n",
              "      <td>0</td>\n",
              "      <td>Optimistic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2021-12-30 21:01:43+00:00</td>\n",
              "      <td>estupefacto pedí hablar con un médico atendió ...</td>\n",
              "      <td>10</td>\n",
              "      <td>Joking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22964</th>\n",
              "      <td>22964</td>\n",
              "      <td>2020-02-15 18:54:10+00:00</td>\n",
              "      <td>china pone en cuarentena los billetes usados p...</td>\n",
              "      <td>8</td>\n",
              "      <td>Official report</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22965</th>\n",
              "      <td>22965</td>\n",
              "      <td>2020-02-15 02:40:38+00:00</td>\n",
              "      <td>febrero coronavirus nivel globalcovid infectad...</td>\n",
              "      <td>8</td>\n",
              "      <td>Official report</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22966</th>\n",
              "      <td>22966</td>\n",
              "      <td>2020-02-13 13:48:34+00:00</td>\n",
              "      <td>nace una nueva enfermedad “ covid ” http co ge...</td>\n",
              "      <td>10</td>\n",
              "      <td>Joking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22967</th>\n",
              "      <td>22967</td>\n",
              "      <td>2020-02-12 03:36:41+00:00</td>\n",
              "      <td>sobre corona virus coronavirus fuente ops de f...</td>\n",
              "      <td>8</td>\n",
              "      <td>Official report</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22968</th>\n",
              "      <td>22968</td>\n",
              "      <td>2020-02-11 19:28:17+00:00</td>\n",
              "      <td>covid http co byzatiscj</td>\n",
              "      <td>0</td>\n",
              "      <td>Optimistic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22969 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84a70628-4155-40e5-8ac5-dc653b66a62c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-84a70628-4155-40e5-8ac5-dc653b66a62c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-84a70628-4155-40e5-8ac5-dc653b66a62c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Unnamed: 0                       Date  \\\n",
              "0               0  2021-12-30 23:46:08+00:00   \n",
              "1               1  2021-12-30 22:43:44+00:00   \n",
              "2               2  2021-12-30 22:17:13+00:00   \n",
              "3               3  2021-12-30 21:01:47+00:00   \n",
              "4               4  2021-12-30 21:01:43+00:00   \n",
              "...           ...                        ...   \n",
              "22964       22964  2020-02-15 18:54:10+00:00   \n",
              "22965       22965  2020-02-15 02:40:38+00:00   \n",
              "22966       22966  2020-02-13 13:48:34+00:00   \n",
              "22967       22967  2020-02-12 03:36:41+00:00   \n",
              "22968       22968  2020-02-11 19:28:17+00:00   \n",
              "\n",
              "                                                   tweet  label  \\\n",
              "0      enserio con toda esto del covid solo deseo ade...      6   \n",
              "1      quiero torta de chocolate puedo comer torta de...     10   \n",
              "2      patriciagamarra hay empresas que saben que su ...     10   \n",
              "3      cuánta gente se contagiará quedará con secuela...      0   \n",
              "4      estupefacto pedí hablar con un médico atendió ...     10   \n",
              "...                                                  ...    ...   \n",
              "22964  china pone en cuarentena los billetes usados p...      8   \n",
              "22965  febrero coronavirus nivel globalcovid infectad...      8   \n",
              "22966  nace una nueva enfermedad “ covid ” http co ge...     10   \n",
              "22967  sobre corona virus coronavirus fuente ops de f...      8   \n",
              "22968                            covid http co byzatiscj      0   \n",
              "\n",
              "                  name  \n",
              "0              Annoyed  \n",
              "1               Joking  \n",
              "2               Joking  \n",
              "3           Optimistic  \n",
              "4               Joking  \n",
              "...                ...  \n",
              "22964  Official report  \n",
              "22965  Official report  \n",
              "22966           Joking  \n",
              "22967  Official report  \n",
              "22968       Optimistic  \n",
              "\n",
              "[22969 rows x 5 columns]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_2 = df_clean_data.copy()\n",
        "label_data(df_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0cbJjCH9mIx"
      },
      "outputs": [],
      "source": [
        "# testing\n",
        "def label_data_multi(df):\n",
        "  size = df.shape[0]\n",
        "  labels = []\n",
        "  names = []\n",
        "  for i in range(10):\n",
        "\n",
        "    text = df.iloc[i]['tweet']\n",
        "\n",
        "    encodings = tokenizer.encode_plus(\n",
        "        text,\n",
        "        None,\n",
        "        add_special_tokens=True,\n",
        "        max_length=MAX_LEN,\n",
        "        padding='max_length',\n",
        "        return_token_type_ids=True,\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "        attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "        token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "        output = model(input_ids, attention_mask, token_type_ids)\n",
        "        final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n",
        "        \n",
        "        min = np.min(final_output[0])\n",
        "        max = np.max(final_output[0])\n",
        "\n",
        "        normalization = (final_output[0] - min)/(max - min)\n",
        "\n",
        "        print()\n",
        "        print(normalization)\n",
        "        print()\n",
        "\n",
        "        index = np.argsort(final_output[0], axis=0)[::-1][:3]\n",
        "\n",
        "        print(index)\n",
        "        print(train_df.columns[1:])\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3sPVz5GD-nm2",
        "outputId": "5d58e97f-aa8f-4b65-db9d-41737a567710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[1.         0.17245483 0.09324784 0.13964621 0.23631182 0.18523865\n",
            " 0.13777579 0.         0.09576996 0.23688321 0.28781122]\n",
            "\n",
            "[ 0 10  9]\n",
            "Index(['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', 'Anxious', 'Sad',\n",
            "       'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'],\n",
            "      dtype='object')\n",
            "\n",
            "[1.         0.30658673 0.12486706 0.0923748  0.0953255  0.11123908\n",
            " 0.1143002  0.         0.02861364 0.21184434 0.44553637]\n",
            "\n",
            "[ 0 10  1]\n",
            "Index(['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', 'Anxious', 'Sad',\n",
            "       'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'],\n",
            "      dtype='object')\n",
            "\n",
            "[0.27609449 0.00791835 0.         0.12392968 0.14575073 0.37345108\n",
            " 0.39397994 0.01832848 0.0058464  0.12651684 1.        ]\n",
            "\n",
            "[10  6  5]\n",
            "Index(['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', 'Anxious', 'Sad',\n",
            "       'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'],\n",
            "      dtype='object')\n",
            "\n",
            "[0.31519736 0.03459523 0.         0.16873684 0.15960697 0.46989919\n",
            " 0.34717956 0.0374597  0.0214495  0.20205946 1.        ]\n",
            "\n",
            "[10  5  6]\n",
            "Index(['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', 'Anxious', 'Sad',\n",
            "       'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'],\n",
            "      dtype='object')\n",
            "\n",
            "[0.38895442 0.06298625 0.         0.26851948 0.55615877 0.41373406\n",
            " 0.22495921 0.22595892 0.53055559 1.         0.53012277]\n",
            "\n",
            "[9 4 8]\n",
            "Index(['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', 'Anxious', 'Sad',\n",
            "       'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'],\n",
            "      dtype='object')\n",
            "\n",
            "[0.29873    0.06039862 0.         0.07805185 0.18605945 0.23727605\n",
            " 0.02908606 0.0382484  1.         0.31112041 0.14410112]\n",
            "\n",
            "[8 9 0]\n",
            "Index(['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', 'Anxious', 'Sad',\n",
            "       'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'],\n",
            "      dtype='object')\n",
            "\n",
            "[0.39318303 0.02814617 0.02427909 0.11151559 0.14822751 0.28538184\n",
            " 0.29465519 0.00149267 0.         0.06252597 1.        ]\n",
            "\n",
            "[10  0  6]\n",
            "Index(['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', 'Anxious', 'Sad',\n",
            "       'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'],\n",
            "      dtype='object')\n",
            "\n",
            "[1.         0.32280494 0.21669645 0.09254241 0.08742142 0.11294301\n",
            " 0.18167926 0.         0.11660619 0.20599519 0.14473942]\n",
            "\n",
            "[0 1 2]\n",
            "Index(['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', 'Anxious', 'Sad',\n",
            "       'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'],\n",
            "      dtype='object')\n",
            "\n",
            "[7.09398117e-02 0.00000000e+00 4.51510239e-05 7.20410325e-02\n",
            " 9.43813735e-02 1.59757660e-01 5.51126089e-02 4.09407845e-02\n",
            " 1.00000000e+00 1.93904207e-01 2.03010572e-02]\n",
            "\n",
            "[8 9 5]\n",
            "Index(['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', 'Anxious', 'Sad',\n",
            "       'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'],\n",
            "      dtype='object')\n",
            "\n",
            "[0.23356307 0.01907349 0.         0.25631883 0.55876382 0.43073676\n",
            " 0.51559177 0.14510047 1.         0.5609184  0.29977401]\n",
            "\n",
            "[8 9 4]\n",
            "Index(['Optimistic', 'Thankful', 'Empathetic', 'Pessimistic', 'Anxious', 'Sad',\n",
            "       'Annoyed', 'Denial', 'Official report', 'Surprise', 'Joking'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-72edf7af-a03b-4ba6-972b-2af98237fbf2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>datetime</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2020-01-06 11:13:36</td>\n",
              "      <td>essaludperu algún día cambiará habrá mayor per...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2020-01-22 16:12:39</td>\n",
              "      <td>caleons jajaja tmre estos tuits alegran el día...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2020-01-26 02:07:56</td>\n",
              "      <td>lo que mete gonzaleszela el trajín voluntad de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>2020-01-27 13:34:12</td>\n",
              "      <td>llegó el coronavirus cusco</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2020-01-27 16:23:16</td>\n",
              "      <td>ya hay posibles portadores del coronavirus en ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2083</th>\n",
              "      <td>2083</td>\n",
              "      <td>2020-12-25 17:38:22</td>\n",
              "      <td>jaajaj fuck creo que tuve covid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2084</th>\n",
              "      <td>2084</td>\n",
              "      <td>2020-12-26 06:30:44</td>\n",
              "      <td>policías murieron en todo el país por causa de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2085</th>\n",
              "      <td>2085</td>\n",
              "      <td>2020-12-29 14:04:47</td>\n",
              "      <td>qué onda con todos los geis que están en rio d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2086</th>\n",
              "      <td>2086</td>\n",
              "      <td>2020-12-30 07:17:17</td>\n",
              "      <td>vizcarra el peor gestor durante la pandemia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2087</th>\n",
              "      <td>2087</td>\n",
              "      <td>2020-12-31 07:06:28</td>\n",
              "      <td>el derecho protestar existe en la marchas hay ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2088 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72edf7af-a03b-4ba6-972b-2af98237fbf2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-72edf7af-a03b-4ba6-972b-2af98237fbf2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-72edf7af-a03b-4ba6-972b-2af98237fbf2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      Unnamed: 0             datetime  \\\n",
              "0              0  2020-01-06 11:13:36   \n",
              "1              1  2020-01-22 16:12:39   \n",
              "2              2  2020-01-26 02:07:56   \n",
              "3              3  2020-01-27 13:34:12   \n",
              "4              4  2020-01-27 16:23:16   \n",
              "...          ...                  ...   \n",
              "2083        2083  2020-12-25 17:38:22   \n",
              "2084        2084  2020-12-26 06:30:44   \n",
              "2085        2085  2020-12-29 14:04:47   \n",
              "2086        2086  2020-12-30 07:17:17   \n",
              "2087        2087  2020-12-31 07:06:28   \n",
              "\n",
              "                                                  tweet  \n",
              "0     essaludperu algún día cambiará habrá mayor per...  \n",
              "1     caleons jajaja tmre estos tuits alegran el día...  \n",
              "2     lo que mete gonzaleszela el trajín voluntad de...  \n",
              "3                            llegó el coronavirus cusco  \n",
              "4     ya hay posibles portadores del coronavirus en ...  \n",
              "...                                                 ...  \n",
              "2083                    jaajaj fuck creo que tuve covid  \n",
              "2084  policías murieron en todo el país por causa de...  \n",
              "2085  qué onda con todos los geis que están en rio d...  \n",
              "2086        vizcarra el peor gestor durante la pandemia  \n",
              "2087  el derecho protestar existe en la marchas hay ...  \n",
              "\n",
              "[2088 rows x 3 columns]"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_11 = df_clean_data.copy()\n",
        "label_data_multi(df_11)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
